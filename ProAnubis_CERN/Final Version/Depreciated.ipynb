{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Cannot activate multiple GUI eventloops\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import h5py\n",
    "import anubisPlotUtils as anPlot\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import hist as hi\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "import matplotlib.colors as colors\n",
    "matplotlib.use('TkAgg')  # or 'Qt5Agg', 'GTK3Agg', etc.\n",
    "import mplhep as hep\n",
    "hep.style.use([hep.style.ATLAS])\n",
    "import sys\n",
    "import AnalysisToolAnubis as AT\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "from pandasgui import show\n",
    "from matplotlib.patches import Circle\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from scipy.signal import convolve2d\n",
    "from scipy.ndimage import label, find_objects\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import sys\n",
    "from scipy.stats import gaussian_kde\n",
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import AnubisSuperScript as ass\n",
    "from functools import partial\n",
    "import importlib\n",
    "from tqdm import tqdm\n",
    "importlib.reload(ass)\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mapping = {\n",
    "    0: {\n",
    "        (0, 31): ('rpc0', 'eta'),\n",
    "        (32, 63): ('rpc0', 'phi1'),\n",
    "        (64, 95): ('rpc0', 'phi2'),\n",
    "        (96, 127): ('rpc1', 'eta'),\n",
    "    },\n",
    "    1: {\n",
    "        (0, 31): ('rpc1', 'phi1'),\n",
    "        (32, 63): ('rpc1', 'phi2'),\n",
    "        (64, 95): ('rpc2', 'eta'),\n",
    "        (96, 127): ('rpc2', 'phi1'),\n",
    "    },\n",
    "    2: {\n",
    "        (0, 31): ('rpc2', 'phi2'),\n",
    "        (32, 63): ('rpc3', 'eta'),\n",
    "        (64, 95): ('rpc3', 'phi1'),\n",
    "        (96, 127): ('rpc3', 'phi2'),\n",
    "    },\n",
    "    3: {\n",
    "        (0, 31): ('rpc4', 'eta'),\n",
    "        (32, 63): ('rpc4', 'phi1'),\n",
    "        (64, 95): ('rpc4', 'phi2'),\n",
    "        (96, 127): ('rpc5', 'eta'),\n",
    "    },\n",
    "    4: {\n",
    "        (0, 31): ('rpc5', 'phi1'),\n",
    "        (32, 63): ('rpc5', 'phi2'),\n",
    "    },\n",
    "}\n",
    "\n",
    "strips_on_steroid = {\n",
    "    'rpc0': {'phi': [0]},\n",
    "    'rpc1': {'phi': [0]},\n",
    "    'rpc2': {'phi': [0]},\n",
    "    'rpc3': {'phi': [0]},\n",
    "    'rpc4': {'eta': [31,0], 'phi': [0]},\n",
    "    'rpc5': {'eta': [31,0], 'phi': [0]},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "thisData = AT.importDatafile('C:\\\\Users\\\\Peter\\\\OneDrive - University of Cambridge\\\\Desktop\\\\Project Excel Work\\\\PartIIIRPC\\\\ProAnubis_CERN\\\\ProAnubisData\\\\proAnubis_240403_0227.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "thisData = [sublist[:94230] for sublist in thisData]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "etaHits, phiHits = ass.divideHitCountsByRPC_Timed(thisData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ass.remake_data(thisData, mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PandasGUI INFO — pandasgui.gui — Opening PandasGUI\n",
      "INFO:pandasgui.gui:Opening PandasGUI\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pandasgui.gui.PandasGui at 0x2ddd9378e50>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show(df.groupby('event number').head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df[df['time'] <= 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df = ass.calculate_cluster_metrics_better(df_clean, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PandasGUI INFO — pandasgui.gui — Opening PandasGUI\n",
      "INFO:pandasgui.gui:Opening PandasGUI\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pandasgui.gui.PandasGui at 0x2dde43bea70>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show(cluster_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5ns window 783898"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_noisy_strips_with_map = partial(ass.remove_noisy_strips, noisy_strips=strips_on_steroid)\n",
    "anti_steriod_df = cluster_df[cluster_df.apply(lambda row: remove_noisy_strips_with_map(row), axis=1)]\n",
    "fully_massaged_df = anti_steriod_df[anti_steriod_df['size'] <= 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PandasGUI INFO — pandasgui.gui — Opening PandasGUI\n",
      "INFO:pandasgui.gui:Opening PandasGUI\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pandasgui.gui.PandasGui at 0x1ddf2028790>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show(fully_massaged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metric_for_combo(combo, rpc_heights):\n",
    "    locations = np.array([c['location'] for c in combo])\n",
    "    heights = np.array([rpc_heights[c['rpc']] for c in combo])\n",
    "    uncertainties = np.array([c['uncertainty'] for c in combo])  \n",
    "    weights = 1 / uncertainties**2\n",
    "\n",
    "    try:\n",
    "        coeffs= np.polyfit(heights, locations, 1, cov=False, w=weights)\n",
    "    except np.linalg.LinAlgError:\n",
    "        coeffs = [np.inf, np.inf]\n",
    "        cov = np.array([[np.inf, np.inf], [np.inf, np.inf]])\n",
    "\n",
    "    slope, intercept = coeffs\n",
    "    # slope_error, intercept_error = np.sqrt(np.diag(cov))\n",
    "    slope_error, intercept_error = 0, 0\n",
    "    predicted = slope * heights + intercept\n",
    "    residuals = locations - predicted\n",
    "    RSS = np.sum(residuals ** 2)\n",
    "\n",
    "    return slope, intercept, slope_error, intercept_error, RSS, combo\n",
    "\n",
    "def analyze_inter_rpc_hit_with_timing_adjusted(df):\n",
    "    rpc_time_offsets = {\n",
    "    ('rpc0', 'eta'): (7.94, 12.48),\n",
    "    ('rpc0', 'phi'): (-2.38, 13.69),\n",
    "    ('rpc1', 'eta'): (8.36, 12.22),\n",
    "    ('rpc1', 'phi'): (-3.79, 13.25),\n",
    "    ('rpc2', 'eta'): (8.84, 12.56),\n",
    "    ('rpc2', 'phi'): (-4.35, 13.57),\n",
    "    ('rpc3', 'eta'): (6.86, 12.41),\n",
    "    ('rpc3', 'phi'): (-4.3, 13.96),\n",
    "    ('rpc4', 'eta'): (2.7, 12.37),\n",
    "    ('rpc4', 'phi'): (-7.89, 13.41),\n",
    "    ('rpc5', 'eta'): (2.82, 13.05),\n",
    "    ('rpc5', 'phi'): (9.15, 14.14),\n",
    "}\n",
    "    paths = []\n",
    "\n",
    "    adjusted_muon_speed_cm_ns = 28\n",
    "\n",
    "    rpc_heights = {\n",
    "        'rpc0': 0, \n",
    "        'rpc1': 0.5, \n",
    "        'rpc2': 1.0, \n",
    "        'rpc3': 61.5, \n",
    "        'rpc4': 121.5, \n",
    "        'rpc5': 122.0\n",
    "    }\n",
    "\n",
    "    for event_number, event_group in tqdm(df.groupby('event_number'), desc=\"Processing Events\"):\n",
    "        for direction in ['eta', 'phi']:\n",
    "            direction_group = event_group[event_group['strip_direction'] == direction]\n",
    "            all_clusters = []\n",
    "\n",
    "            unique_rpcs = direction_group['rpc_number'].unique()\n",
    "            for rpc in unique_rpcs:\n",
    "                rpc_group = direction_group[direction_group['rpc_number'] == rpc]\n",
    "                for _, row in rpc_group.iterrows():\n",
    "                    location_scaling = 3.09375 if direction == 'eta' else 2.8125\n",
    "                    strip_locations = np.array(row['locations'])\n",
    "                    non_zero_locations = strip_locations[strip_locations != 0]\n",
    "                    if non_zero_locations.size > 0:\n",
    "                        strip_location = non_zero_locations[0] \n",
    "                    else:\n",
    "                        continue \n",
    "\n",
    "                    location = strip_location * location_scaling\n",
    "                    event_time = np.mean(row['times']) - rpc_time_offsets[(rpc, direction)][0]\n",
    "                    cluster_size_scaled = max(row['size'] * location_scaling, location_scaling) / 2\n",
    "                    all_clusters.append({\n",
    "                        'rpc': rpc,\n",
    "                        'location': location,\n",
    "                        'event_time': event_time,\n",
    "                        'uncertainty': cluster_size_scaled,\n",
    "                        'original_location': strip_location\n",
    "                    })\n",
    "\n",
    "            combination_metrics = []\n",
    "            valid_combinations = [] \n",
    "            # for n in range(3, 4): \n",
    "            for combo in combinations(all_clusters, 3):\n",
    "                used_rpcs_set = {c['rpc'] for c in combo}\n",
    "                \n",
    "                if not used_rpcs_set.issubset({'rpc0', 'rpc1', 'rpc2'}):\n",
    "                    metric = calculate_metric_for_combo(combo, rpc_heights)\n",
    "        \n",
    "                    # Check RSS threshold here and ensure RPC combination uniqueness\n",
    "                    if metric[4] <= 10 and len(used_rpcs_set) == len(combo):\n",
    "                        combination_metrics.append(metric)\n",
    "                        \n",
    "            # Move filtering logic outside the loop so it's not reset each time\n",
    "            for combo_metric in combination_metrics:\n",
    "                combo = combo_metric[-1]\n",
    "                if len({c['rpc'] for c in combo}) < len(combo):\n",
    "                    continue\n",
    "\n",
    "                time_diffs_are_valid = True\n",
    "                for i in range(len(combo)):\n",
    "                    for j in range(i + 1, len(combo)):\n",
    "                        error_window = rpc_time_offsets[(combo[i]['rpc'], direction)][1] + rpc_time_offsets[(combo[j]['rpc'], direction)][1]\n",
    "                        \n",
    "                        # Use direct height differences\n",
    "                        height_diff = abs(rpc_heights[combo[i]['rpc']] - rpc_heights[combo[j]['rpc']])\n",
    "                        \n",
    "                        time_diff = abs(combo[i]['event_time'] - combo[j]['event_time'])\n",
    "                        expected_time_diff = height_diff / adjusted_muon_speed_cm_ns\n",
    "\n",
    "                        uncertainty_margin = 5\n",
    "                        # Use the expected time difference with the error window and uncertainty margin for validation\n",
    "                        if not (time_diff <= expected_time_diff + error_window + uncertainty_margin):\n",
    "                            time_diffs_are_valid = False\n",
    "                            break\n",
    "                    if not time_diffs_are_valid:\n",
    "                        break\n",
    "\n",
    "                if time_diffs_are_valid:\n",
    "                    valid_combinations.append(combo_metric)\n",
    "\n",
    "            for valid_combination in valid_combinations:\n",
    "                paths.append({\n",
    "                    'Event Number': event_number,\n",
    "                    'Direction': direction,\n",
    "                    'Slope': valid_combination[0],\n",
    "                    'Intercept': valid_combination[1],\n",
    "                    'Slope_error': valid_combination[2],\n",
    "                    'Intercept_error': valid_combination[3],\n",
    "                    'Used Coordinates': [(c['rpc'], c['original_location'], c['event_time']) for c in valid_combination[-1]],\n",
    "                    'RSS': valid_combination[4],\n",
    "                })\n",
    "\n",
    "    path_df = pd.DataFrame(paths)\n",
    "    return path_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Events:   0%|          | 11/94217 [00:00<14:17, 109.83it/s]C:\\Users\\Peter\\AppData\\Local\\Temp\\ipykernel_32180\\1308193378.py:8: RankWarning:\n",
      "\n",
      "Polyfit may be poorly conditioned\n",
      "\n",
      "Processing Events: 100%|██████████| 94217/94217 [05:34<00:00, 281.31it/s]\n"
     ]
    }
   ],
   "source": [
    "Reconstruct2 = analyze_inter_rpc_hit_with_timing_adjusted(fully_massaged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PandasGUI INFO — pandasgui.gui — Opening PandasGUI\n",
      "INFO:pandasgui.gui:Opening PandasGUI\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pandasgui.gui.PandasGui at 0x1dda630d750>"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show(Reconstruct2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PandasGUI INFO — pandasgui.gui — Opening PandasGUI\n",
      "INFO:pandasgui.gui:Opening PandasGUI\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pandasgui.gui.PandasGui at 0x1ddc4e4f250>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show(fully_massaged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Peter\\AppData\\Local\\Temp\\ipykernel_32180\\3657440907.py:3: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        event_number mean_location_str\n",
      "1193             444       [  0. -17.]\n",
      "2143             774       [  0. -24.]\n",
      "3188            1131       [  0. -31.]\n",
      "3819            1356       [  0. -24.]\n",
      "4085            1450         [ 0. -2.]\n",
      "...              ...               ...\n",
      "263764         91632       [  0. -31.]\n",
      "264592         91918       [  0. -23.]\n",
      "265020         92058       [  0. -25.]\n",
      "265425         92187       [  0. -21.]\n",
      "265488         92213       [  0. -22.]\n",
      "\n",
      "[452 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Peter\\AppData\\Local\\Temp\\ipykernel_32180\\3657440907.py:5: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_etaaaa = fully_massaged_df[fully_massaged_df['strip_direction'] == 'eta']\n",
    "\n",
    "df_etaaaa['mean_location'] = df_etaaaa['locations'].apply(lambda x: np.mean(x, axis=0))\n",
    "\n",
    "df_etaaaa['mean_location_str'] = df_etaaaa['mean_location'].apply(lambda x: str(x))\n",
    "\n",
    "resultaaa = df_etaaaa.groupby(['event_number', 'mean_location_str']).size().reset_index(name='counts')\n",
    "\n",
    "# Filtering events where the same mean location appeared more than 3 times\n",
    "filtered_resultaaa = resultaaa[resultaaa['counts'] > 3]\n",
    "\n",
    "# Outputting the final result\n",
    "print(filtered_resultaaa[['event_number', 'mean_location_str']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'ReconNewWeighing.xlsx'\n",
    "\n",
    "Reconstruct2.to_excel(file_path, index=False, engine='openpyxl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filtered_reconstruct = Reconstruct[Reconstruct['RSS'] < 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-33.690067525979785\n"
     ]
    }
   ],
   "source": [
    "print(np.degrees(np.arctan(1/5)) - 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'C:\\\\Users\\\\Peter\\\\OneDrive - University of Cambridge\\\\Desktop\\\\Project Excel Work\\\\PartIIIRPC\\\\ProAnubis_CERN\\\\Final Version\\\\ReconOldWeighing.xlsx'\n",
    "\n",
    "df_Old = pd.read_excel(file_path, sheet_name=0)\n",
    "\n",
    "file_path2 = 'C:\\\\Users\\\\Peter\\\\OneDrive - University of Cambridge\\\\Desktop\\\\Project Excel Work\\\\PartIIIRPC\\\\ProAnubis_CERN\\\\Final Version\\\\ReconNewWeighing.xlsx'\n",
    "\n",
    "df_New = pd.read_excel(file_path2, sheet_name=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PandasGUI INFO — pandasgui.gui — Opening PandasGUI\n",
      "INFO:pandasgui.gui:Opening PandasGUI\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pandasgui.gui.PandasGui at 0x22d865c8670>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show(df_New.head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filtered_df_Old = df_Old[df_Old['RSS'] < 10]\n",
    "Filtered_df_New = df_New[df_New['RSS'] < 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Event Number Direction     Slope   Intercept  Slope_error  Intercept_error  \\\n",
      "0             1       phi  0.138889  120.867008            0                0   \n",
      "1             3       eta  0.353554  -31.025528            0                0   \n",
      "2             3       eta  0.358126  -31.307596            0                0   \n",
      "3             3       eta -0.051111   -6.137904            0                0   \n",
      "4             3       phi -2.112209  143.963397            0                0   \n",
      "\n",
      "                                    Used Coordinates       RSS  \n",
      "0  [('rpc1', 43, 241.79), ('rpc3', 46, 238.8), ('...  0.001136  \n",
      "1  [('rpc0', -10, 249.06), ('rpc1', -10, 249.64),...  0.015626  \n",
      "2  [('rpc1', -10, 249.64), ('rpc2', -11, 250.16),...  9.533868  \n",
      "3  [('rpc1', -2, 251.64), ('rpc2', -2, 251.16), (...  0.000580  \n",
      "4  [('rpc0', 51, 250.04666666666665), ('rpc1', 51...  0.557688  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 334027 entries, 0 to 334026\n",
      "Data columns (total 8 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   Event Number      334027 non-null  int64  \n",
      " 1   Direction         334027 non-null  object \n",
      " 2   Slope             334027 non-null  float64\n",
      " 3   Intercept         334027 non-null  float64\n",
      " 4   Slope_error       334027 non-null  int64  \n",
      " 5   Intercept_error   334027 non-null  int64  \n",
      " 6   Used Coordinates  334027 non-null  object \n",
      " 7   RSS               334027 non-null  float64\n",
      "dtypes: float64(3), int64(3), object(2)\n",
      "memory usage: 22.9+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(Filtered_df_New.head())\n",
    "\n",
    "print(Filtered_df_New.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ties(sub_df):\n",
    "    if len(sub_df) > 1:\n",
    "        avg_slope = sub_df['Slope'].mean()\n",
    "        avg_intercept = sub_df['Intercept'].mean()\n",
    "        sub_df = sub_df.head(1) \n",
    "        sub_df['Slope'] = avg_slope \n",
    "        sub_df['Intercept'] = avg_intercept\n",
    "    return sub_df\n",
    "\n",
    "Filtered_df_Old = (\n",
    "    df_Old.sort_values(by=['Event Number', 'Direction', 'RSS'])\n",
    "      .groupby(['Event Number', 'Direction'], as_index=False)\n",
    "      .apply(lambda x: process_ties(x[x['RSS'] == x['RSS'].min()]))\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "Filtered_df_New = (\n",
    "    df_New.sort_values(by=['Event Number', 'Direction', 'RSS'])\n",
    "      .groupby(['Event Number', 'Direction'], as_index=False)\n",
    "      .apply(lambda x: process_ties(x[x['RSS'] == x['RSS'].min()]))\n",
    "      .reset_index(drop=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filtered_df_Old = Filtered_df_Old[Filtered_df_Old['RSS'] < 3]\n",
    "Filtered_df_New = Filtered_df_New[Filtered_df_New['RSS'] < 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PandasGUI INFO — pandasgui.gui — Opening PandasGUI\n",
      "INFO:pandasgui.gui:Opening PandasGUI\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pandasgui.gui.PandasGui at 0x22dd2b9c4c0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show(Filtered_df_Old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_df = Filtered_df_New[Filtered_df_New['Direction'] == 'phi']\n",
    "angles_y_axis = np.degrees(np.arctan(-eta_df['Slope']))\n",
    "\n",
    "plt.figure(figsize=(20, 12))\n",
    "plt.hist(angles_y_axis, bins=100, color='skyblue', edgecolor='black')\n",
    "plt.xlabel('angles (degrees)')\n",
    "plt.ylabel('Occurance')\n",
    "plt.title('Cross_Chamber_Angular_Distribution_eta')\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_df = Filtered_df_Old[Filtered_df_Old['Direction'] == 'phi']\n",
    "angles = np.degrees(np.arctan(phi_df['Slope']))\n",
    "\n",
    "plt.figure(figsize=(20, 12))\n",
    "plt.hist(angles, bins=100, color='skyblue', edgecolor='black')\n",
    "plt.xlabel('angles (degrees)')\n",
    "plt.ylabel('Occurance')\n",
    "plt.title('Cross_Chamber_Angular_Distribution_phi')\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_eta = Filtered_reconstruct[Filtered_reconstruct['Direction'] == 'eta']['Event Number'].unique()\n",
    "has_phi = Filtered_reconstruct[Filtered_reconstruct['Direction'] == 'phi']['Event Number'].unique()\n",
    "events_with_both = np.intersect1d(has_eta, has_phi)\n",
    "\n",
    "filtered_df = Filtered_reconstruct[Filtered_reconstruct['Event Number'].isin(events_with_both)]\n",
    "\n",
    "eta_df = filtered_df[filtered_df['Direction'] == 'eta']\n",
    "angles_y_axis = np.degrees(np.arctan(-eta_df['Slope']))\n",
    "weights = 1 / eta_df.groupby('Event Number')['Event Number'].transform('count')\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(20, 12))\n",
    "plt.hist(angles_y_axis, bins=100, weights=weights, color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Angles (degrees)')\n",
    "plt.ylabel('Weighted Occurance')\n",
    "plt.title('Cross_Chamber_Angular_Distribution_eta (weighted)')\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Filtered_reconstruct' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m has_eta \u001b[38;5;241m=\u001b[39m \u001b[43mFiltered_reconstruct\u001b[49m[Filtered_reconstruct[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDirection\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meta\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent Number\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\n\u001b[0;32m      2\u001b[0m has_phi \u001b[38;5;241m=\u001b[39m Filtered_reconstruct[Filtered_reconstruct[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDirection\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mphi\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent Number\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\n\u001b[0;32m      3\u001b[0m events_with_both \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mintersect1d(has_eta, has_phi)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Filtered_reconstruct' is not defined"
     ]
    }
   ],
   "source": [
    "has_eta = Filtered_reconstruct[Filtered_reconstruct['Direction'] == 'eta']['Event Number'].unique()\n",
    "has_phi = Filtered_reconstruct[Filtered_reconstruct['Direction'] == 'phi']['Event Number'].unique()\n",
    "events_with_both = np.intersect1d(has_eta, has_phi)\n",
    "\n",
    "filtered_df = Filtered_reconstruct[Filtered_reconstruct['Event Number'].isin(events_with_both)]\n",
    "\n",
    "eta_df = filtered_df[filtered_df['Direction'] == 'phi']\n",
    "angles_y_axis = np.degrees(np.arctan(eta_df['Slope']))\n",
    "weights = 1 / eta_df.groupby('Event Number')['Event Number'].transform('count')\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(20, 12))\n",
    "plt.hist(angles_y_axis, bins=100, weights=weights, color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Angles (degrees)')\n",
    "plt.ylabel('Weighted Occurance')\n",
    "plt.title('Cross_Chamber_Angular_Distribution_phi (weighted)')\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_eta1 = Filtered_df_Old[Filtered_df_Old['Direction'] == 'eta']['Event Number'].unique()\n",
    "has_phi1 = Filtered_df_Old[Filtered_df_Old['Direction'] == 'phi']['Event Number'].unique()\n",
    "events_with_both1 = np.intersect1d(has_eta1, has_phi1)\n",
    "\n",
    "filtered_df1 = Filtered_df_Old[Filtered_df_Old['Event Number'].isin(events_with_both1)]\n",
    "\n",
    "eta_df1 = filtered_df1[filtered_df1['Direction'] == 'eta']\n",
    "phi_df1 = filtered_df1[filtered_df1['Direction'] == 'phi']\n",
    "angles_y_axis1 = np.degrees(np.arctan(-eta_df1['Slope']))\n",
    "weights1 = 1 / eta_df1.groupby('Event Number')['Event Number'].transform('count')\n",
    "\n",
    "has_eta2 = Filtered_df_New[Filtered_df_New['Direction'] == 'eta']['Event Number'].unique()\n",
    "has_phi2 = Filtered_df_New[Filtered_df_New['Direction'] == 'phi']['Event Number'].unique()\n",
    "events_with_both2 = np.intersect1d(has_eta2, has_phi2)\n",
    "\n",
    "filtered_df2 = Filtered_df_New[Filtered_df_New['Event Number'].isin(events_with_both2)]\n",
    "\n",
    "eta_df2 = filtered_df2[filtered_df2['Direction'] == 'eta']\n",
    "phi_df2 = filtered_df2[filtered_df2['Direction'] == 'phi']\n",
    "angles_y_axis2 = np.degrees(np.arctan(-eta_df2['Slope']))\n",
    "weights2 = 1 / eta_df2.groupby('Event Number')['Event Number'].transform('count')\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(20, 12))\n",
    "plt.hist(angles_y_axis2, bins=1000, weights=weights2, color='pink', edgecolor='green', alpha = 1, label = 'Atlas On')\n",
    "plt.hist(angles_y_axis1, bins=1000, weights=weights1, color='skyblue', edgecolor='black', alpha = 0.5, label = 'Atlas Off')\n",
    "plt.xlabel('Angles (degrees)')\n",
    "plt.ylabel('Weighted Occurance')\n",
    "plt.title('Cross_Chamber_Angular_Distribution_eta (weighted)')\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_eta1 = Filtered_df_Old[Filtered_df_Old['Direction'] == 'eta']['Event Number'].unique()\n",
    "has_phi1 = Filtered_df_Old[Filtered_df_Old['Direction'] == 'phi']['Event Number'].unique()\n",
    "events_with_both1 = np.intersect1d(has_eta1, has_phi1)\n",
    "\n",
    "filtered_df1 = Filtered_df_Old[Filtered_df_Old['Event Number'].isin(events_with_both1)]\n",
    "\n",
    "eta_df1 = Filtered_df_Old[Filtered_df_Old['Direction'] == 'eta']\n",
    "phi_df1 = Filtered_df_Old[Filtered_df_Old['Direction'] == 'phi']\n",
    "angles_y_axis1 = np.degrees(np.arctan(-eta_df1['Slope']))\n",
    "angles_y_axis11 = np.degrees(np.arctan(phi_df1['Slope']))\n",
    "weights1 = 1 / eta_df1.groupby('Event Number')['Event Number'].transform('count')\n",
    "weights11 = 1 / phi_df1.groupby('Event Number')['Event Number'].transform('count')\n",
    "\n",
    "has_eta2 = Filtered_df_New[Filtered_df_New['Direction'] == 'eta']['Event Number'].unique()\n",
    "has_phi2 = Filtered_df_New[Filtered_df_New['Direction'] == 'phi']['Event Number'].unique()\n",
    "events_with_both2 = np.intersect1d(has_eta2, has_phi2)\n",
    "\n",
    "filtered_df2 = Filtered_df_New[Filtered_df_New['Event Number'].isin(events_with_both2)]\n",
    "\n",
    "eta_df2 = Filtered_df_New[Filtered_df_New['Direction'] == 'eta']\n",
    "phi_df2 = Filtered_df_New[Filtered_df_New['Direction'] == 'phi']\n",
    "angles_y_axis2 = np.degrees(np.arctan(-eta_df2['Slope']))\n",
    "angles_y_axis22 = np.degrees(np.arctan(phi_df2['Slope']))\n",
    "weights2 = 1 / eta_df2.groupby('Event Number')['Event Number'].transform('count')\n",
    "weights22 = 1 / phi_df2.groupby('Event Number')['Event Number'].transform('count')\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(20, 12))\n",
    "plt.hist(angles_y_axis2, bins=100, weights=weights2, color='pink', edgecolor='green', alpha = 1, label = 'Atlas On')\n",
    "plt.hist(angles_y_axis1, bins=100, weights=weights1, color='skyblue', edgecolor='black', alpha = 0.5, label = 'Atlas Off')\n",
    "plt.xlabel('Angles (degrees)')\n",
    "plt.ylabel('Weighted Occurance')\n",
    "plt.title('Cross_Chamber_Angular_Distribution_eta (weighted)')\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20, 12))\n",
    "plt.hist(angles_y_axis22, bins=100, weights=weights22, color='pink', edgecolor='green', alpha = 1, label = 'Atlas On')\n",
    "plt.hist(angles_y_axis11, bins=100, weights=weights11, color='skyblue', edgecolor='black', alpha = 0.5, label = 'Atlas Off')\n",
    "plt.xlabel('Angles (degrees)')\n",
    "plt.ylabel('Weighted Occurance')\n",
    "plt.title('Cross_Chamber_Angular_Distribution_phi (weighted)')\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_eta1 = Filtered_df_Old[Filtered_df_Old['Direction'] == 'eta']['Event Number'].unique()\n",
    "has_phi1 = Filtered_df_Old[Filtered_df_Old['Direction'] == 'phi']['Event Number'].unique()\n",
    "events_with_both1 = np.intersect1d(has_eta1, has_phi1)\n",
    "\n",
    "filtered_df1 = Filtered_df_Old\n",
    "\n",
    "eta_df1 = Filtered_df_Old[Filtered_df_Old['Direction'] == 'eta']\n",
    "phi_df1 = Filtered_df_Old[Filtered_df_Old['Direction'] == 'phi']\n",
    "angles_y_axis1 = np.degrees(np.arctan(-eta_df1['Slope']))\n",
    "angles_y_axis11 = np.degrees(np.arctan(-phi_df1['Slope']))\n",
    "weights1 = 1 / eta_df1.groupby('Event Number')['Event Number'].transform('count')\n",
    "weights11 = 1 / phi_df1.groupby('Event Number')['Event Number'].transform('count')\n",
    "\n",
    "has_eta2 = Filtered_df_New[Filtered_df_New['Direction'] == 'eta']['Event Number'].unique()\n",
    "has_phi2 = Filtered_df_New[Filtered_df_New['Direction'] == 'phi']['Event Number'].unique()\n",
    "events_with_both2 = np.intersect1d(has_eta2, has_phi2)\n",
    "\n",
    "filtered_df2 = Filtered_df_New[Filtered_df_New['Event Number'].isin(events_with_both2)]\n",
    "\n",
    "eta_df2 = Filtered_df_New[Filtered_df_New['Direction'] == 'eta']\n",
    "phi_df2 = Filtered_df_New[Filtered_df_New['Direction'] == 'phi']\n",
    "angles_y_axis2 = np.degrees(np.arctan(-eta_df2['Slope']))\n",
    "angles_y_axis22 = np.degrees(np.arctan(phi_df2['Slope']))\n",
    "weights2 = 1 / eta_df2.groupby('Event Number')['Event Number'].transform('count')\n",
    "weights22 = 1 / phi_df2.groupby('Event Number')['Event Number'].transform('count')\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(20, 12))\n",
    "plt.hist(angles_y_axis2, bins=100, weights=weights2, color='pink', edgecolor='green', alpha = 1, label = 'Atlas On')\n",
    "plt.hist(angles_y_axis1, bins=100, weights=weights1, color='skyblue', edgecolor='black', alpha = 0.5, label = 'Atlas Off')\n",
    "plt.xlabel('Angles (degrees)')\n",
    "plt.ylabel('Weighted Occurance')\n",
    "plt.title('Cross_Chamber_Angular_Distribution_eta (weighted)')\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20, 12))\n",
    "plt.hist(angles_y_axis22, bins=100, weights=weights22, color='pink', edgecolor='green', alpha = 1, label = 'Atlas On')\n",
    "plt.hist(angles_y_axis11, bins=100, weights=weights11, color='skyblue', edgecolor='black', alpha = 0.5, label = 'Atlas Off')\n",
    "plt.xlabel('Angles (degrees)')\n",
    "plt.ylabel('Weighted Occurance')\n",
    "plt.title('Cross_Chamber_Angular_Distribution_phi (weighted)')\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.25755884e-02 -7.27031509e+01]\n",
      "4.80517744748802\n"
     ]
    }
   ],
   "source": [
    "y = np.array([-24* 3.09375, -23 * 3.09375, -24 * 3.09375])\n",
    "x = np.array([0, 0.5, 122.5])\n",
    "z = np.polyfit(x, y, 1)\n",
    "print(z)\n",
    "predicted = z[0] * x + z[1]\n",
    "residuals = y - predicted\n",
    "RSS = np.sum(residuals ** 2)\n",
    "print(RSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_times(row):\n",
    "    coords = ast.literal_eval(row['Used Coordinates'])\n",
    "    times = {0: [], 1: [], 2: []}\n",
    "\n",
    "    for coord in coords:\n",
    "        rpc_id = int(coord[0][3])  \n",
    "        time = coord[2]\n",
    "        if rpc_id in [0, 1, 2]:\n",
    "            times[0].append(time)\n",
    "        elif rpc_id == 3:\n",
    "            times[1].append(time)\n",
    "        elif rpc_id in [4, 5]:\n",
    "            times[2].append(time)\n",
    "\n",
    "    for key in times:\n",
    "        if times[key]:\n",
    "            times[key] = sum(times[key]) / len(times[key])\n",
    "        else:\n",
    "            times[key] = None  \n",
    "\n",
    "    return times\n",
    "\n",
    "def determine_direction(times):\n",
    "    t1, t2, t3 = times[0], times[1], times[2]\n",
    "\n",
    "    if t1 is not None and t3 is not None:\n",
    "        return 'bottom' if t1 < t3 else 'top'\n",
    "    elif t1 is not None and t2 is not None:\n",
    "        return 'bottom' if t1 < t2 else 'top'\n",
    "    elif t2 is not None and t3 is not None:\n",
    "        return 'bottom' if t2 < t3 else 'top'\n",
    "    else:\n",
    "        return 'unknown'  \n",
    "\n",
    "Filtered_df_New['Parsed Times'] = Filtered_df_New.apply(parse_times, axis=1)\n",
    "Filtered_df_New['Muon Direction'] = Filtered_df_New['Parsed Times'].apply(determine_direction)\n",
    "Filtered_df_New.drop(columns=['Parsed Times'], inplace=True)\n",
    "Filtered_df_Old['Parsed Times'] = Filtered_df_Old.apply(parse_times, axis=1)\n",
    "Filtered_df_Old['Muon Direction'] = Filtered_df_Old['Parsed Times'].apply(determine_direction)\n",
    "Filtered_df_Old.drop(columns=['Parsed Times'], inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df, direction):\n",
    "    subset = df[df['Direction'] == direction]\n",
    "    angles = np.degrees(np.arctan(subset['Slope']))\n",
    "    weights = 1 / subset.groupby('Event Number')['Event Number'].transform('count')\n",
    "    # Separate the weights for top and bottom muons\n",
    "    top_angles = angles[subset['Muon Direction'] == 'top']\n",
    "    top_weights = weights[subset['Muon Direction'] == 'top']\n",
    "    bottom_angles = angles[subset['Muon Direction'] == 'bottom']\n",
    "    bottom_weights = -weights[subset['Muon Direction'] == 'bottom']  # Note the negative sign for bottom\n",
    "    return top_angles, top_weights, bottom_angles, bottom_weights\n",
    "\n",
    "angles_eta_new_top, weights_eta_new_top, angles_eta_new_bottom, weights_eta_new_bottom = prepare_data(Filtered_df_New, 'eta')\n",
    "angles_eta_old_top, weights_eta_old_top, angles_eta_old_bottom, weights_eta_old_bottom = prepare_data(Filtered_df_Old, 'eta')\n",
    "angles_phi_new_top, weights_phi_new_top, angles_phi_new_bottom, weights_phi_new_bottom = prepare_data(Filtered_df_New, 'phi')\n",
    "angles_phi_old_top, weights_phi_old_top, angles_phi_old_bottom, weights_phi_old_bottom = prepare_data(Filtered_df_Old, 'phi')\n",
    "\n",
    "# Plotting all on the same plot\n",
    "plt.figure(figsize=(20, 12))\n",
    "plt.hist(angles_eta_old_top, bins=50, weights=weights_eta_old_top, color='skyblue', alpha=0.75, label='Old (Top)')\n",
    "plt.hist(angles_eta_new_top, bins=50, weights=weights_eta_new_top, color='pink', alpha=0.75, label='AtlasOn (Top)')\n",
    "plt.hist(angles_eta_new_bottom, bins=50, weights=weights_eta_new_bottom, color='red', alpha=0.75, label='AtlasOn (Bottom)')\n",
    "plt.hist(angles_eta_old_bottom, bins=50, weights=weights_eta_old_bottom, color='blue', alpha=0.75, label='Old (Bottom)')\n",
    "plt.xlabel('Angles (degrees)')\n",
    "plt.ylabel('Weighted Occurrence (Positive for Top, Negative for Bottom)')\n",
    "plt.title('Cross_Chamber_Angular_Distribution_eta (weighted)')\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20, 12))\n",
    "plt.hist(angles_phi_old_top, bins=50, weights=weights_phi_old_top, color='skyblue', alpha=0.75, label='Old (Top)')\n",
    "plt.hist(angles_phi_new_top, bins=50, weights=weights_phi_new_top, color='pink', alpha=0.75, label='AtlasOn (Top)')\n",
    "plt.hist(angles_phi_new_bottom, bins=50, weights=weights_phi_new_bottom, color='red', alpha=0.75, label='AtlasOn (Bottom)')\n",
    "plt.hist(angles_phi_old_bottom, bins=50, weights=weights_phi_old_bottom, color='blue', alpha=0.75, label='Old (Bottom)')\n",
    "plt.xlabel('Angles (degrees)')\n",
    "plt.ylabel('Weighted Occurrence (Positive for Top, Negative for Bottom)')\n",
    "plt.title('Cross_Chamber_Angular_Distribution_phi (weighted)')\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     False         Decay\n",
      "Event Number                            \n",
      "1             {rpc4, rpc2}            {}\n",
      "3                       {}  {rpc4, rpc5}\n",
      "7                       {}            {}\n",
      "10                      {}  {rpc4, rpc5}\n",
      "11                      {}            {}\n",
      "...                    ...           ...\n",
      "94190               {rpc3}        {rpc5}\n",
      "94193                   {}            {}\n",
      "94198                   {}  {rpc4, rpc5}\n",
      "94204                   {}            {}\n",
      "94223                   {}            {}\n",
      "\n",
      "[34168 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "def parse_used_coordinates(coord_input):\n",
    "    # Check if the input is a string that needs parsing\n",
    "    if isinstance(coord_input, str):\n",
    "        return ast.literal_eval(coord_input)\n",
    "    # If it's already the right format (list or set), return it directly\n",
    "    return coord_input\n",
    "\n",
    "# Re-define the aggregate_paths function to properly aggregate data\n",
    "def aggregate_paths(df):\n",
    "    # Ensure coordinates are parsed correctly\n",
    "    df['Parsed Coordinates'] = df['Used Coordinates'].apply(parse_used_coordinates)\n",
    "    # Aggregate all unique RPCs observed in all paths within each event\n",
    "    aggregated_data = df.groupby('Event Number').agg({\n",
    "        'Parsed Coordinates': lambda x: {rpc[0] for sublist in x for rpc in sublist},\n",
    "        'Muon Direction': 'first'  # Assuming all entries for a single event have the same direction\n",
    "    })\n",
    "    return aggregated_data\n",
    "\n",
    "def analyze_event(row):\n",
    "    observed_rpcs = row['Parsed Coordinates']\n",
    "    direction = row['Muon Direction']\n",
    "    expected_sequence = rpc_sequence if direction == 'bottom' else list(reversed(rpc_sequence))\n",
    "\n",
    "    false_detection = set()\n",
    "    decay_detection = set()\n",
    "\n",
    "    # Get indices of observed RPCs in the expected sequence\n",
    "    observed_indices = [expected_sequence.index(rpc) for rpc in observed_rpcs if rpc in expected_sequence]\n",
    "    min_observed_index = min(observed_indices, default=None)\n",
    "    max_observed_index = max(observed_indices, default=None)\n",
    "\n",
    "    for i, rpc in enumerate(expected_sequence):\n",
    "        # Sandwiched False logic\n",
    "        if rpc not in observed_rpcs:\n",
    "            if i > 0 and i < len(expected_sequence) - 1:\n",
    "                if expected_sequence[i - 1] in observed_rpcs and expected_sequence[i + 1] in observed_rpcs:\n",
    "                    false_detection.add(rpc)\n",
    "\n",
    "    # Decay detection logic\n",
    "    if direction == 'top' and min_observed_index is not None:\n",
    "        # Mark RPCs above the first detected RPC as decay\n",
    "        for i in range(min_observed_index):\n",
    "            decay_detection.add(expected_sequence[i])\n",
    "    elif direction == 'bottom' and max_observed_index is not None:\n",
    "        # Mark RPCs below the last detected RPC as decay\n",
    "        for i in range(max_observed_index + 1, len(expected_sequence)):\n",
    "            decay_detection.add(expected_sequence[i])\n",
    "\n",
    "    return pd.Series({'False': false_detection, 'Decay': decay_detection})\n",
    "\n",
    "rpc_sequence = ['rpc0', 'rpc1', 'rpc2', 'rpc3', 'rpc4', 'rpc5']\n",
    "aggregated_df = aggregate_paths(Filtered_df_New)\n",
    "results = aggregated_df.apply(analyze_event, axis=1)\n",
    "aggregated_df = pd.concat([aggregated_df, results], axis=1)\n",
    "aggregated_df2 = aggregate_paths(Filtered_df_Old)\n",
    "results2 = aggregated_df2.apply(analyze_event, axis=1)\n",
    "aggregated_df2 = pd.concat([aggregated_df2, results2], axis=1)\n",
    "\n",
    "print(aggregated_df[['False', 'Decay']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PandasGUI INFO — pandasgui.gui — Opening PandasGUI\n",
      "INFO:pandasgui.gui:Opening PandasGUI\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pandasgui.gui.PandasGui at 0x22f99996b00>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show(aggregated_df2.head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PandasGUI INFO — pandasgui.gui — Opening PandasGUI\n",
      "INFO:pandasgui.gui:Opening PandasGUI\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pandasgui.gui.PandasGui at 0x22d8d687910>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show(df.head(10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def union_sets(series):\n",
    "    return set().union(*series)\n",
    "\n",
    "# Group by Event Number and union the sets\n",
    "grouped_false1 = aggregated_df.groupby('Event Number')['False'].apply(union_sets).explode().reset_index()\n",
    "grouped_false2 = aggregated_df2.groupby('Event Number')['False'].apply(union_sets).explode().reset_index()\n",
    "grouped_decay = aggregated_df2.groupby('Event Number')['Decay'].apply(union_sets).explode().reset_index()\n",
    "\n",
    "# Count occurrences\n",
    "false_counts1 = grouped_false1['False'].value_counts()\n",
    "false_counts2 = grouped_false2['False'].value_counts()\n",
    "decay_counts = grouped_decay['Decay'].value_counts()\n",
    "\n",
    "# Total number of events (using the index of the grouped DataFrames)\n",
    "total_events1 = grouped_false1.index.nunique()\n",
    "total_events2 = grouped_false2.index.nunique()\n",
    "\n",
    "# Calculate inefficiency (False) and decay rates\n",
    "false_rates1 = false_counts1 / total_events1\n",
    "false_rates2 = false_counts2 / total_events2\n",
    "# decay_rates = decay_counts / total_events\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(2, 1, figsize=(10, 12))\n",
    "false_rates1.plot(kind='bar', ax=ax[0], color='red', title='RPC FAILED Detection Rate Atlas On')\n",
    "false_rates2.plot(kind='bar', ax=ax[1], color='green', title='RPC FAILED Detection Rate Atlas OFF')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rpc2    4.747776\n",
      "rpc0    4.613439\n",
      "rpc1    4.487093\n",
      "rpc3    3.844708\n",
      "rpc4    2.241513\n",
      "Name: False, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(false_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PandasGUI INFO — pandasgui.gui — Opening PandasGUI\n",
      "INFO:pandasgui.gui:Opening PandasGUI\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pandasgui.gui.PandasGui at 0x22fb7798d30>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show(Filtered_df_New.head(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I am going to go Barack all the way to the data, and find more insight on timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fully_massaged_df = cluster_df[cluster_df['size'] <= 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 815112 entries, 0 to 839726\n",
      "Data columns (total 8 columns):\n",
      " #   Column           Non-Null Count   Dtype \n",
      "---  ------           --------------   ----- \n",
      " 0   event_number     815112 non-null  int64 \n",
      " 1   rpc_number       815112 non-null  object\n",
      " 2   strip_direction  815112 non-null  object\n",
      " 3   locations        815112 non-null  object\n",
      " 4   times            815112 non-null  object\n",
      " 5   start_time       815112 non-null  int64 \n",
      " 6   end_time         815112 non-null  int64 \n",
      " 7   size             815112 non-null  int64 \n",
      "dtypes: int64(4), object(4)\n",
      "memory usage: 56.0+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(fully_massaged_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PandasGUI INFO — pandasgui.gui — Opening PandasGUI\n",
      "INFO:pandasgui.gui:Opening PandasGUI\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pandasgui.gui.PandasGui at 0x2062a836d40>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show(fully_massaged_df.head(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metric_for_combo(combo, rpc_heights):\n",
    "    locations = np.array([c['location'] for c in combo])\n",
    "    heights = np.array([rpc_heights[c['rpc']] for c in combo])\n",
    "    uncertainties = np.array([c['uncertainty'] for c in combo])  \n",
    "    weights = 1 / uncertainties**2\n",
    "\n",
    "    try:\n",
    "        coeffs= np.polyfit(heights, locations, 1, cov=False, w=weights)\n",
    "    except np.linalg.LinAlgError:\n",
    "        coeffs = [np.inf, np.inf]\n",
    "        # cov = np.array([[np.inf, np.inf], [np.inf, np.inf]])\n",
    "\n",
    "    slope, intercept = coeffs\n",
    "    # slope_error, intercept_error = np.sqrt(np.diag(cov))\n",
    "    slope_error, intercept_error = 0, 0\n",
    "    predicted = slope * heights + intercept\n",
    "    residuals = locations - predicted\n",
    "    RSS = np.sum(residuals ** 2)\n",
    "\n",
    "    return slope, intercept, slope_error, intercept_error, RSS, combo\n",
    "\n",
    "def analyze_inter_rpc_hit_with_timing_adjusted(df,n):\n",
    "    rpc_time_offsets = {\n",
    "    ('rpc0', 'eta'): (7.94, 12.48),\n",
    "    ('rpc0', 'phi'): (-2.38, 13.69),\n",
    "    ('rpc1', 'eta'): (8.36, 12.22),\n",
    "    ('rpc1', 'phi'): (-3.79, 13.25),\n",
    "    ('rpc2', 'eta'): (8.84, 12.56),\n",
    "    ('rpc2', 'phi'): (-4.35, 13.57),\n",
    "    ('rpc3', 'eta'): (6.86, 12.41),\n",
    "    ('rpc3', 'phi'): (-4.3, 13.96),\n",
    "    ('rpc4', 'eta'): (2.7, 12.37),\n",
    "    ('rpc4', 'phi'): (-7.89, 13.41),\n",
    "    ('rpc5', 'eta'): (2.82, 13.05),\n",
    "    ('rpc5', 'phi'): (9.15, 14.14),\n",
    "}\n",
    "    paths = []\n",
    "\n",
    "    adjusted_muon_speed_cm_ns = 28\n",
    "\n",
    "    rpc_heights = {\n",
    "        'rpc0': 0, \n",
    "        'rpc1': 0.5, \n",
    "        'rpc2': 1.0, \n",
    "        'rpc3': 61.5, \n",
    "        'rpc4': 121.5, \n",
    "        'rpc5': 122.0\n",
    "    }\n",
    "\n",
    "    for event_number, event_group in tqdm(df.groupby('event_number'), desc=\"Processing Events\"):\n",
    "        for direction in ['eta', 'phi']:\n",
    "            direction_group = event_group[event_group['strip_direction'] == direction]\n",
    "            all_clusters = []\n",
    "\n",
    "            unique_rpcs = direction_group['rpc_number'].unique()\n",
    "            if len(unique_rpcs) < n:\n",
    "                break\n",
    "            for rpc in unique_rpcs:\n",
    "                rpc_group = direction_group[direction_group['rpc_number'] == rpc]\n",
    "                for _, row in rpc_group.iterrows():\n",
    "                    location_scaling = 3.09375 if direction == 'eta' else 2.8125\n",
    "                    strip_locations = np.array(row['locations'])\n",
    "                    non_zero_locations = strip_locations[strip_locations != 0]\n",
    "                    if non_zero_locations.size > 0:\n",
    "                        strip_location = non_zero_locations[0] \n",
    "                    else:\n",
    "                        continue \n",
    "\n",
    "                    location = strip_location * location_scaling\n",
    "                    event_time = np.mean(row['times']) - rpc_time_offsets[(rpc, direction)][0]\n",
    "                    cluster_size_scaled = max(row['size'] * location_scaling, location_scaling) / 2\n",
    "                    all_clusters.append({\n",
    "                        'rpc': rpc,\n",
    "                        'location': location,\n",
    "                        'event_time': event_time,\n",
    "                        'uncertainty': cluster_size_scaled,\n",
    "                        'original_location': strip_location\n",
    "                    })\n",
    "\n",
    "            combination_metrics = []\n",
    "            valid_combinations = [] \n",
    "            \n",
    "            for combo in combinations(all_clusters, n):\n",
    "                if len({c['rpc'] for c in combo}) != len(combo):\n",
    "                    continue  # Skip combinations where RPCs are not unique\n",
    "\n",
    "                time_diffs_are_valid = True\n",
    "                for i in range(len(combo)):\n",
    "                    for j in range(i + 1, len(combo)):\n",
    "                        error_window = rpc_time_offsets[(combo[i]['rpc'], direction)][1] + rpc_time_offsets[(combo[j]['rpc'], direction)][1]\n",
    "                        height_diff = abs(rpc_heights[combo[i]['rpc']] - rpc_heights[combo[j]['rpc']])\n",
    "                        time_diff = abs(combo[i]['event_time'] - combo[j]['event_time'])\n",
    "                        expected_time_diff = height_diff / adjusted_muon_speed_cm_ns\n",
    "                        uncertainty_margin = 5\n",
    "\n",
    "                        if not (time_diff <= expected_time_diff + error_window + uncertainty_margin):\n",
    "                            time_diffs_are_valid = False\n",
    "                            break\n",
    "                    if not time_diffs_are_valid:\n",
    "                        break\n",
    "\n",
    "                if not time_diffs_are_valid:\n",
    "                    continue  # Skip to the next combination if time differences are invalid\n",
    "\n",
    "                # Calculate metrics only for valid combinations\n",
    "                metric = calculate_metric_for_combo(combo, rpc_heights)\n",
    "                if metric[4] <= 10:\n",
    "                    valid_combinations.append(metric)\n",
    "                    \n",
    "\n",
    "            for valid_combination in valid_combinations:\n",
    "                paths.append({\n",
    "                    'Event Number': event_number,\n",
    "                    'Direction': direction,\n",
    "                    'Slope': valid_combination[0],\n",
    "                    'Intercept': valid_combination[1],\n",
    "                    'Slope_error': valid_combination[2],\n",
    "                    'Intercept_error': valid_combination[3],\n",
    "                    'Used Coordinates': [(c['rpc'], c['original_location'], c['event_time']) for c in valid_combination[-1]],\n",
    "                    'RSS': valid_combination[4],\n",
    "                })\n",
    "\n",
    "    path_df = pd.DataFrame(paths)\n",
    "    return path_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_rpc0 = fully_massaged_df[fully_massaged_df['rpc_number'] != 'rpc0']\n",
    "df_no_rpc1 = fully_massaged_df[fully_massaged_df['rpc_number'] != 'rpc1']\n",
    "df_no_rpc2 = fully_massaged_df[fully_massaged_df['rpc_number'] != 'rpc2']\n",
    "df_no_rpc3 = fully_massaged_df[fully_massaged_df['rpc_number'] != 'rpc3']\n",
    "df_no_rpc4 = fully_massaged_df[fully_massaged_df['rpc_number'] != 'rpc4']\n",
    "df_no_rpc5 = fully_massaged_df[fully_massaged_df['rpc_number'] != 'rpc5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PandasGUI INFO — pandasgui.gui — Opening PandasGUI\n",
      "INFO:pandasgui.gui:Opening PandasGUI\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pandasgui.gui.PandasGui at 0x2067f650b80>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show(df_no_rpc0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Events: 100%|██████████| 94229/94229 [04:41<00:00, 334.68it/s]\n",
      "Processing Events: 100%|██████████| 94229/94229 [03:36<00:00, 435.36it/s] \n",
      "Processing Events: 100%|██████████| 94229/94229 [05:38<00:00, 278.34it/s] \n",
      "Processing Events: 100%|██████████| 94229/94229 [09:32<00:00, 164.50it/s] \n"
     ]
    }
   ],
   "source": [
    "df_recon3 = analyze_inter_rpc_hit_with_timing_adjusted(fully_massaged_df, 3)\n",
    "df_recon4 = analyze_inter_rpc_hit_with_timing_adjusted(fully_massaged_df, 4)\n",
    "df_recon5 = analyze_inter_rpc_hit_with_timing_adjusted(fully_massaged_df, 5)\n",
    "df_recon6 = analyze_inter_rpc_hit_with_timing_adjusted(fully_massaged_df, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recon3.to_excel('df_recon3_AtlasOn_2304.xlsx')\n",
    "df_recon4.to_excel('df_recon4_AtlasOn_2304.xlsx')\n",
    "df_recon5.to_excel('df_recon5_AtlasOn_2304.xlsx')\n",
    "df_recon6.to_excel('df_recon6_AtlasOn_2304.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recon6_filtered = df_recon6.groupby(['Event Number', 'Direction'], group_keys=False) \\\n",
    "              .apply(lambda x: x.sort_values('RSS').iloc[:-1]) \\\n",
    "              .groupby(['Event Number', 'Direction']).head(1)\n",
    "\n",
    "df_recon5_filtered = df_recon5.groupby(['Event Number', 'Direction'], group_keys=False) \\\n",
    "              .apply(lambda x: x.sort_values('RSS').iloc[:-1]) \\\n",
    "              .groupby(['Event Number', 'Direction']).head(1)\n",
    "\n",
    "df_recon4_filtered = df_recon4.groupby(['Event Number', 'Direction'], group_keys=False) \\\n",
    "              .apply(lambda x: x.sort_values('RSS').iloc[:-1]) \\\n",
    "              .groupby(['Event Number', 'Direction']).head(1)\n",
    "              \n",
    "df_recon3_filtered = df_recon3.groupby(['Event Number', 'Direction'], group_keys=False) \\\n",
    "              .apply(lambda x: x.sort_values('RSS').iloc[:-1]) \\\n",
    "              .groupby(['Event Number', 'Direction']).head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_event_types(df):\n",
    "    event_directions = {}\n",
    "    counts = np.array([0, 0, 0])\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        event_number = row['Event Number']\n",
    "        direction = row['Direction']\n",
    "\n",
    "        if event_number not in event_directions:\n",
    "            event_directions[event_number] = set()\n",
    "        \n",
    "        event_directions[event_number].add(direction)\n",
    "\n",
    "    for directions in event_directions.values():\n",
    "        if directions == {'eta'}:\n",
    "            counts[0] += 1\n",
    "        elif directions == {'phi'}:\n",
    "            counts[1] += 1\n",
    "        elif directions == {'eta', 'phi'}:\n",
    "            counts[2] += 1\n",
    "\n",
    "    return counts\n",
    "\n",
    "# result = count_event_types(df_recon6_filtered)\n",
    "# print(\"Counts [Only Eta, Only Phi, Both Eta and Phi]:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "result6 = count_event_types(df_recon6_filtered)\n",
    "result5 = count_event_types(df_recon5_filtered)\n",
    "result4 = count_event_types(df_recon4_filtered)\n",
    "result3 = count_event_types(df_recon3_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficiency6 = result6[2] / (result6[0] + result6[2] + result6[1])\n",
    "efficiency5 = result5[2] / (result5[0] + result5[2] + result5[1])\n",
    "efficiency4 = result4[2] / (result4[0] + result4[2] + result4[1])\n",
    "efficiency3 = result3[2] / (result3[0] + result3[2] + result3[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_labels = [3, 4, 5, 6]\n",
    "\n",
    "efficiencies = [efficiency3, efficiency4, efficiency5, efficiency6]\n",
    "\n",
    "plt.bar(x_labels, efficiencies, color='blue')\n",
    "plt.title('eta phi Efficiency/number of plates Plot')\n",
    "plt.xlabel('plate used for reconstruction')\n",
    "plt.ylabel('Efficiency')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Events: 100%|██████████| 94193/94193 [01:19<00:00, 1191.91it/s]\n",
      "Processing Events: 100%|██████████| 94212/94212 [01:08<00:00, 1382.54it/s]\n",
      "Processing Events: 100%|██████████| 94217/94217 [01:16<00:00, 1238.40it/s]\n",
      "Processing Events: 100%|██████████| 94180/94180 [01:10<00:00, 1334.13it/s]\n",
      "Processing Events: 100%|██████████| 94212/94212 [01:29<00:00, 1055.00it/s]\n",
      "Processing Events: 100%|██████████| 94226/94226 [01:21<00:00, 1162.84it/s]\n"
     ]
    }
   ],
   "source": [
    "df_recon_no_rpc0_4 = analyze_inter_rpc_hit_with_timing_adjusted(df_no_rpc0, 5)\n",
    "df_recon_no_rpc1_4 = analyze_inter_rpc_hit_with_timing_adjusted(df_no_rpc1, 5)\n",
    "df_recon_no_rpc2_4 = analyze_inter_rpc_hit_with_timing_adjusted(df_no_rpc2, 5)\n",
    "df_recon_no_rpc3_4 = analyze_inter_rpc_hit_with_timing_adjusted(df_no_rpc3, 5)\n",
    "df_recon_no_rpc4_4 = analyze_inter_rpc_hit_with_timing_adjusted(df_no_rpc4, 5)\n",
    "df_recon_no_rpc5_4 = analyze_inter_rpc_hit_with_timing_adjusted(df_no_rpc5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PandasGUI INFO — pandasgui.gui — Opening PandasGUI\n",
      "INFO:pandasgui.gui:Opening PandasGUI\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pandasgui.gui.PandasGui at 0x2ddd9b7f7f0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show(df_recon_no_rpc5_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recon_no_rpc0_4.to_excel('df_recon_no_rpc0_5_AtlasOn_2304.xlsx')\n",
    "df_recon_no_rpc1_4.to_excel('df_recon_no_rpc1_5_AtlasOn_2304.xlsx')\n",
    "df_recon_no_rpc2_4.to_excel('df_recon_no_rpc2_5_AtlasOn_2304.xlsx')\n",
    "df_recon_no_rpc3_4.to_excel('df_recon_no_rpc3_5_AtlasOn_2304.xlsx')\n",
    "df_recon_no_rpc4_4.to_excel('df_recon_no_rpc4_5_AtlasOn_2304.xlsx')\n",
    "df_recon_no_rpc5_4.to_excel('df_recon_no_rpc5_5_AtlasOn_2304.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PandasGUI INFO — pandasgui.gui — Opening PandasGUI\n",
      "INFO:pandasgui.gui:Opening PandasGUI\n",
      "C:\\Users\\Peter\\AppData\\Local\\Temp\\ipykernel_53540\\1082192393.py:1: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  show(df_recon_no_rpc0_4)\n",
      "C:\\Users\\Peter\\AppData\\Local\\Temp\\ipykernel_53540\\1082192393.py:1: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  show(df_recon_no_rpc0_4)\n",
      "C:\\Users\\Peter\\AppData\\Local\\Temp\\ipykernel_53540\\1082192393.py:1: FutureWarning:\n",
      "\n",
      "The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "\n",
      "C:\\Users\\Peter\\AppData\\Local\\Temp\\ipykernel_53540\\1082192393.py:1: FutureWarning:\n",
      "\n",
      "The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "\n",
      "C:\\Users\\Peter\\AppData\\Local\\Temp\\ipykernel_53540\\1082192393.py:1: FutureWarning:\n",
      "\n",
      "The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pandasgui.gui.PandasGui at 0x2de9856c9d0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show(df_recon_no_rpc0_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recon_no_rpc0_4 = pd.read_excel('df_recon_no_rpc0_4_AtlasOn.xlsx')\n",
    "df_recon_no_rpc1_4 = pd.read_excel('df_recon_no_rpc1_4_AtlasOn.xlsx')\n",
    "df_recon_no_rpc2_4 = pd.read_excel('df_recon_no_rpc2_4_AtlasOn.xlsx')\n",
    "df_recon_no_rpc3_4 = pd.read_excel('df_recon_no_rpc3_4_AtlasOn.xlsx')\n",
    "df_recon_no_rpc4_4 = pd.read_excel('df_recon_no_rpc4_4_AtlasOn.xlsx')\n",
    "df_recon_no_rpc5_4 = pd.read_excel('df_recon_no_rpc5_4_AtlasOn.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recon_no_rpc0_4_filtered = df_recon_no_rpc0_4.groupby(['Event Number', 'Direction'], group_keys=False) \\\n",
    "              .apply(lambda x: x.sort_values('RSS').iloc[:-1]) \\\n",
    "              .groupby(['Event Number', 'Direction']).head(1)\n",
    "\n",
    "df_recon_no_rpc1_4_filtered = df_recon_no_rpc1_4.groupby(['Event Number', 'Direction'], group_keys=False) \\\n",
    "              .apply(lambda x: x.sort_values('RSS').iloc[:-1]) \\\n",
    "              .groupby(['Event Number', 'Direction']).head(1)\n",
    "\n",
    "df_recon_no_rpc2_4_filtered = df_recon_no_rpc2_4.groupby(['Event Number', 'Direction'], group_keys=False) \\\n",
    "              .apply(lambda x: x.sort_values('RSS').iloc[:-1]) \\\n",
    "              .groupby(['Event Number', 'Direction']).head(1)\n",
    "              \n",
    "df_recon_no_rpc3_4_filtered = df_recon_no_rpc3_4.groupby(['Event Number', 'Direction'], group_keys=False) \\\n",
    "              .apply(lambda x: x.sort_values('RSS').iloc[:-1]) \\\n",
    "              .groupby(['Event Number', 'Direction']).head(1)\n",
    "\n",
    "df_recon_no_rpc4_4_filtered = df_recon_no_rpc4_4.groupby(['Event Number', 'Direction'], group_keys=False) \\\n",
    "              .apply(lambda x: x.sort_values('RSS').iloc[:-1]) \\\n",
    "              .groupby(['Event Number', 'Direction']).head(1)\n",
    "              \n",
    "df_recon_no_rpc5_4_filtered = df_recon_no_rpc5_4.groupby(['Event Number', 'Direction'], group_keys=False) \\\n",
    "              .apply(lambda x: x.sort_values('RSS').iloc[:-1]) \\\n",
    "              .groupby(['Event Number', 'Direction']).head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recon_no_rpc0_4_filtered['expected hits'] = df_recon_no_rpc0_4_filtered.apply(lambda row: (row['Slope'] * 0 + row['Intercept']) / 3.09375 if row['Direction'] == 'eta' \n",
    "                               else (row['Slope'] * 0 + row['Intercept']) / 2.8125 if row['Direction'] == 'phi' \n",
    "                               else None, axis=1)\n",
    "df_recon_no_rpc1_4_filtered['expected hits'] = df_recon_no_rpc1_4_filtered.apply(lambda row: (row['Slope'] * 0 + row['Intercept']) / 3.09375 if row['Direction'] == 'eta' \n",
    "                               else (row['Slope'] * 0 + row['Intercept']) / 2.8125 if row['Direction'] == 'phi' \n",
    "                               else None, axis=1)\n",
    "df_recon_no_rpc2_4_filtered['expected hits'] = df_recon_no_rpc2_4_filtered.apply(lambda row: (row['Slope'] * 0 + row['Intercept']) / 3.09375 if row['Direction'] == 'eta' \n",
    "                               else (row['Slope'] * 0 + row['Intercept']) / 2.8125 if row['Direction'] == 'phi' \n",
    "                               else None, axis=1)\n",
    "df_recon_no_rpc3_4_filtered['expected hits'] = df_recon_no_rpc3_4_filtered.apply(lambda row: (row['Slope'] * 0 + row['Intercept']) / 3.09375 if row['Direction'] == 'eta' \n",
    "                               else (row['Slope'] * 0 + row['Intercept']) / 2.8125 if row['Direction'] == 'phi' \n",
    "                               else None, axis=1)\n",
    "df_recon_no_rpc4_4_filtered['expected hits'] = df_recon_no_rpc4_4_filtered.apply(lambda row: (row['Slope'] * 0 + row['Intercept']) / 3.09375 if row['Direction'] == 'eta' \n",
    "                               else (row['Slope'] * 0 + row['Intercept']) / 2.8125 if row['Direction'] == 'phi' \n",
    "                               else None, axis=1)\n",
    "df_recon_no_rpc5_4_filtered['expected hits'] = df_recon_no_rpc5_4_filtered.apply(lambda row: (row['Slope'] * 0 + row['Intercept']) / 3.09375 if row['Direction'] == 'eta' \n",
    "                               else (row['Slope'] * 0 + row['Intercept']) / 2.8125 if row['Direction'] == 'phi' \n",
    "                               else None, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PandasGUI INFO — pandasgui.gui — Opening PandasGUI\n",
      "INFO:pandasgui.gui:Opening PandasGUI\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pandasgui.gui.PandasGui at 0x239072020e0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show(df_recon_no_rpc0_4_filtered.head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PandasGUI INFO — pandasgui.gui — Opening PandasGUI\n",
      "INFO:pandasgui.gui:Opening PandasGUI\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pandasgui.gui.PandasGui at 0x2079a364040>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show(fully_massaged_df.head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Peter\\AppData\\Local\\Temp\\ipykernel_53540\\2424636361.py:16: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\Peter\\AppData\\Local\\Temp\\ipykernel_53540\\2424636361.py:17: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\Peter\\AppData\\Local\\Temp\\ipykernel_53540\\2424636361.py:18: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\Peter\\AppData\\Local\\Temp\\ipykernel_53540\\2424636361.py:19: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\Peter\\AppData\\Local\\Temp\\ipykernel_53540\\2424636361.py:20: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\Peter\\AppData\\Local\\Temp\\ipykernel_53540\\2424636361.py:21: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fully_massaged_df_rpc0 = fully_massaged_df[fully_massaged_df['rpc_number'] == 'rpc0']\n",
    "fully_massaged_df_rpc1 = fully_massaged_df[fully_massaged_df['rpc_number'] == 'rpc1']\n",
    "fully_massaged_df_rpc2 = fully_massaged_df[fully_massaged_df['rpc_number'] == 'rpc2']\n",
    "fully_massaged_df_rpc3 = fully_massaged_df[fully_massaged_df['rpc_number'] == 'rpc3']\n",
    "fully_massaged_df_rpc4 = fully_massaged_df[fully_massaged_df['rpc_number'] == 'rpc4']\n",
    "fully_massaged_df_rpc5 = fully_massaged_df[fully_massaged_df['rpc_number'] == 'rpc5']\n",
    "\n",
    "def calculate_midpoint(locations, direction):\n",
    "    if direction == 'eta':\n",
    "        return np.mean([loc[1] for loc in locations])\n",
    "    elif direction == 'phi':\n",
    "        return np.mean([loc[0] for loc in locations])\n",
    "    return None\n",
    "\n",
    "# Adding a column to rpc0_df2 for the midpoint\n",
    "fully_massaged_df_rpc0['midpoint'] = fully_massaged_df_rpc0.apply(lambda row: calculate_midpoint(row['locations'], row['strip_direction']), axis=1)\n",
    "fully_massaged_df_rpc1['midpoint'] = fully_massaged_df_rpc1.apply(lambda row: calculate_midpoint(row['locations'], row['strip_direction']), axis=1)\n",
    "fully_massaged_df_rpc2['midpoint'] = fully_massaged_df_rpc2.apply(lambda row: calculate_midpoint(row['locations'], row['strip_direction']), axis=1)\n",
    "fully_massaged_df_rpc3['midpoint'] = fully_massaged_df_rpc3.apply(lambda row: calculate_midpoint(row['locations'], row['strip_direction']), axis=1)\n",
    "fully_massaged_df_rpc4['midpoint'] = fully_massaged_df_rpc4.apply(lambda row: calculate_midpoint(row['locations'], row['strip_direction']), axis=1)\n",
    "fully_massaged_df_rpc5['midpoint'] = fully_massaged_df_rpc5.apply(lambda row: calculate_midpoint(row['locations'], row['strip_direction']), axis=1)\n",
    "\n",
    "# Merging dfs to compare\n",
    "merged_df0 = pd.merge(df_recon_no_rpc0_4_filtered, fully_massaged_df_rpc0, left_on=['Event Number', 'Direction'], right_on=['event_number', 'strip_direction'], how='left')\n",
    "merged_df1 = pd.merge(df_recon_no_rpc1_4_filtered, fully_massaged_df_rpc1, left_on=['Event Number', 'Direction'], right_on=['event_number', 'strip_direction'], how='left')\n",
    "merged_df2 = pd.merge(df_recon_no_rpc2_4_filtered, fully_massaged_df_rpc2, left_on=['Event Number', 'Direction'], right_on=['event_number', 'strip_direction'], how='left')\n",
    "merged_df3 = pd.merge(df_recon_no_rpc3_4_filtered, fully_massaged_df_rpc3, left_on=['Event Number', 'Direction'], right_on=['event_number', 'strip_direction'], how='left')\n",
    "merged_df4 = pd.merge(df_recon_no_rpc4_4_filtered, fully_massaged_df_rpc4, left_on=['Event Number', 'Direction'], right_on=['event_number', 'strip_direction'], how='left')\n",
    "merged_df5 = pd.merge(df_recon_no_rpc5_4_filtered, fully_massaged_df_rpc5, left_on=['Event Number', 'Direction'], right_on=['event_number', 'strip_direction'], how='left')\n",
    "\n",
    "\n",
    "merged_df0['hit_difference'] = merged_df0['expected hits'] - merged_df0['midpoint']\n",
    "merged_df1['hit_difference'] = merged_df1['expected hits'] - merged_df1['midpoint']\n",
    "merged_df2['hit_difference'] = merged_df2['expected hits'] - merged_df2['midpoint']\n",
    "merged_df3['hit_difference'] = merged_df3['expected hits'] - merged_df3['midpoint']\n",
    "merged_df4['hit_difference'] = merged_df4['expected hits'] - merged_df4['midpoint']\n",
    "merged_df5['hit_difference'] = merged_df5['expected hits'] - merged_df5['midpoint']\n",
    "\n",
    "result_df0 = merged_df0[['Event Number', 'Direction', 'expected hits', 'rpc_number', 'midpoint', 'hit_difference']]\n",
    "result_df1 = merged_df1[['Event Number', 'Direction', 'expected hits', 'rpc_number', 'midpoint', 'hit_difference']]\n",
    "result_df2 = merged_df2[['Event Number', 'Direction', 'expected hits', 'rpc_number', 'midpoint', 'hit_difference']]\n",
    "result_df3 = merged_df3[['Event Number', 'Direction', 'expected hits', 'rpc_number', 'midpoint', 'hit_difference']]\n",
    "result_df4 = merged_df4[['Event Number', 'Direction', 'expected hits', 'rpc_number', 'midpoint', 'hit_difference']]\n",
    "result_df5 = merged_df5[['Event Number', 'Direction', 'expected hits', 'rpc_number', 'midpoint', 'hit_difference']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_df0 = result_df0[result_df0['Direction'] == 'eta']\n",
    "eta_df1 = result_df1[result_df1['Direction'] == 'eta']\n",
    "eta_df2 = result_df2[result_df2['Direction'] == 'eta']\n",
    "eta_df3 = result_df3[result_df3['Direction'] == 'eta']\n",
    "eta_df4 = result_df4[result_df4['Direction'] == 'eta']\n",
    "eta_df5 = result_df5[result_df5['Direction'] == 'eta']\n",
    "\n",
    "phi_df0 = result_df0[result_df0['Direction'] == 'phi']\n",
    "phi_df1 = result_df1[result_df1['Direction'] == 'phi']\n",
    "phi_df2 = result_df2[result_df2['Direction'] == 'phi']\n",
    "phi_df3 = result_df3[result_df3['Direction'] == 'phi']\n",
    "phi_df4 = result_df4[result_df4['Direction'] == 'phi']\n",
    "phi_df5 = result_df5[result_df5['Direction'] == 'phi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_min_index(group):\n",
    "    if group['hit_difference'].isna().all():\n",
    "        return group.index.tolist()  # Return all indices if all are NaN\n",
    "    return [group['hit_difference'].idxmin()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_df0 = phi_df0.loc[phi_df0.groupby('Event Number').apply(get_min_index).explode().astype(int)]\n",
    "phi_df1 = phi_df1.loc[phi_df1.groupby('Event Number').apply(get_min_index).explode().astype(int)]\n",
    "phi_df2 = phi_df2.loc[phi_df2.groupby('Event Number').apply(get_min_index).explode().astype(int)]\n",
    "phi_df3 = phi_df3.loc[phi_df3.groupby('Event Number').apply(get_min_index).explode().astype(int)]\n",
    "phi_df4 = phi_df4.loc[phi_df4.groupby('Event Number').apply(get_min_index).explode().astype(int)]\n",
    "phi_df5 = phi_df5.loc[phi_df5.groupby('Event Number').apply(get_min_index).explode().astype(int)]\n",
    "\n",
    "eta_df0 = eta_df0.loc[eta_df0.groupby('Event Number').apply(get_min_index).explode().astype(int)]\n",
    "eta_df1 = eta_df1.loc[eta_df1.groupby('Event Number').apply(get_min_index).explode().astype(int)]\n",
    "eta_df2 = eta_df2.loc[eta_df2.groupby('Event Number').apply(get_min_index).explode().astype(int)]\n",
    "eta_df3 = eta_df3.loc[eta_df3.groupby('Event Number').apply(get_min_index).explode().astype(int)]\n",
    "eta_df4 = eta_df4.loc[eta_df4.groupby('Event Number').apply(get_min_index).explode().astype(int)]\n",
    "eta_df5 = eta_df5.loc[eta_df5.groupby('Event Number').apply(get_min_index).explode().astype(int)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Event Number Direction  expected hits rpc_number  midpoint  \\\n",
      "0               31       eta     -13.943812       rpc0     -14.0   \n",
      "1               51       eta     -24.960487       rpc0     -24.0   \n",
      "2              129       eta     -23.982698       rpc0     -24.0   \n",
      "3              157       eta      -6.959865       rpc0      -7.0   \n",
      "5              161       eta      -6.895650        NaN       NaN   \n",
      "...            ...       ...            ...        ...       ...   \n",
      "2364         92107       eta     -14.028003       rpc0     -14.0   \n",
      "2365         92161       eta      -6.746919       rpc0     -31.0   \n",
      "2366         92161       eta      -6.746919       rpc0      -5.0   \n",
      "2367         92161       eta      -6.746919       rpc0     -19.0   \n",
      "2370         92238       eta     -19.055707       rpc0     -19.0   \n",
      "\n",
      "      hit_difference  \n",
      "0           0.056188  \n",
      "1          -0.960487  \n",
      "2           0.017302  \n",
      "3           0.040135  \n",
      "5                NaN  \n",
      "...              ...  \n",
      "2364       -0.028003  \n",
      "2365       24.253081  \n",
      "2366       -1.746919  \n",
      "2367       12.253081  \n",
      "2370       -0.055707  \n",
      "\n",
      "[1399 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(eta_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = range(33)  \n",
    "\n",
    "eta_df = [eta_df0, eta_df1, eta_df2, eta_df3, eta_df4, eta_df5]\n",
    "\n",
    "efficiencies = [[],[],[],[],[],[]]\n",
    "\n",
    "label_rpc = ['missing rpc0', 'missing rpc1', 'missing rpc2', 'missing rpc3', 'missing rpc4', 'missing rpc5']\n",
    "\n",
    "for i in range(6):\n",
    "    for threshold in thresholds:\n",
    "        valid_hits = eta_df[i].dropna(subset=['hit_difference'])\n",
    "\n",
    "        count_valid = valid_hits[(valid_hits['hit_difference'].abs() <= threshold) & (~valid_hits['hit_difference'].isna())].shape[0]\n",
    "\n",
    "        total_valid = eta_df[i].shape[0]\n",
    "\n",
    "        efficiency = count_valid / total_valid if total_valid > 0 else 0\n",
    "        efficiencies[i].append(efficiency)\n",
    "\n",
    "plt.figure(figsize=(20, 12))\n",
    "for i in range(6):\n",
    "    plt.plot(thresholds, efficiencies[i], marker='o', label = label_rpc[i])\n",
    "plt.title('Efficiency vs Hit Difference Threshold for ETA Direction RPC0 any 4 AtlasOn')\n",
    "plt.xlabel('Hit Difference Threshold in Number of Strips')\n",
    "plt.ylabel('Efficiency')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = range(63)  \n",
    "\n",
    "phi_df = [phi_df0, phi_df1, phi_df2, phi_df3, phi_df4, phi_df5]\n",
    "\n",
    "efficiencies = [[],[],[],[],[],[]]\n",
    "\n",
    "label_rpc = ['missing rpc0', 'missing rpc1', 'missing rpc2', 'missing rpc3', 'missing rpc4', 'missing rpc5']\n",
    "\n",
    "for i in range(6):\n",
    "    for threshold in thresholds:\n",
    "        valid_hits = phi_df[i].dropna(subset=['hit_difference'])\n",
    "\n",
    "        count_valid = valid_hits[(valid_hits['hit_difference'].abs() <= threshold) & (~valid_hits['hit_difference'].isna())].shape[0]\n",
    "\n",
    "        total_valid = phi_df[i].shape[0]\n",
    "\n",
    "        efficiency = count_valid / total_valid if total_valid > 0 else 0\n",
    "        efficiencies[i].append(efficiency)\n",
    "\n",
    "plt.figure(figsize=(20, 12))\n",
    "for i in range(6):\n",
    "    plt.plot(thresholds, efficiencies[i], marker='o', label = label_rpc[i])\n",
    "plt.title('Efficiency vs Hit Difference Threshold for PHI Direction RPC0 any 4 Atlas On')\n",
    "plt.xlabel('Hit Difference Threshold in Number of Strips')\n",
    "plt.ylabel('Efficiency')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 129 entries, 0 to 339\n",
      "Data columns (total 6 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Event Number    129 non-null    int64  \n",
      " 1   Direction       129 non-null    object \n",
      " 2   expected hits   129 non-null    float64\n",
      " 3   rpc_number      124 non-null    object \n",
      " 4   midpoint        124 non-null    float64\n",
      " 5   hit_difference  124 non-null    float64\n",
      "dtypes: float64(3), int64(1), object(2)\n",
      "memory usage: 7.1+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(phi_df1.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PandasGUI INFO — pandasgui.gui — Opening PandasGUI\n",
      "INFO:pandasgui.gui:Opening PandasGUI\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pandasgui.gui.PandasGui at 0x2079a533760>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show(phi_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PandasGUI INFO — pandasgui.gui — Opening PandasGUI\n",
      "INFO:pandasgui.gui:Opening PandasGUI\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pandasgui.gui.PandasGui at 0x207992ffeb0>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = [[],[],[],[],[],[]]\n",
    "\n",
    "efficiencies = [[],[],[],[],[],[]]\n",
    "\n",
    "label_rpc = ['missing rpc0', 'missing rpc1', 'missing rpc2', 'missing rpc3', 'missing rpc4', 'missing rpc5']\n",
    "\n",
    "thresholds = range(70)  \n",
    "\n",
    "for i in range(6):\n",
    "    combined_df[i] = pd.merge(eta_df[i], phi_df[i], on='Event Number', how='inner')\n",
    "    \n",
    "    \n",
    "for i in range(6):\n",
    "    for threshold in thresholds:\n",
    "        valid_hits = combined_df[i].dropna(subset=['hit_difference_x', 'hit_difference_y'], how='any')\n",
    "\n",
    "        # Calculate the number of valid hits within the threshold\n",
    "        count_valid = valid_hits[((valid_hits['hit_difference_x'].abs()) ** 2 + (valid_hits['hit_difference_y'].abs()) ** 2 <= threshold ** 2)].shape[0]\n",
    "\n",
    "        # Total number of rows after dropping NaN values\n",
    "        total_valid = combined_df[i].shape[0]\n",
    "\n",
    "        # Calculate efficiency\n",
    "        \n",
    "        efficiency = count_valid / total_valid if total_valid > 0 else 0\n",
    "        efficiencies[i].append(efficiency)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20, 12))\n",
    "for i in range(6):\n",
    "    plt.plot(thresholds, efficiencies[i], marker='o', label = label_rpc[i])\n",
    "plt.title('Efficiency vs Hit Difference Threshold for Eta Phi Combined any 4 Atlas On')\n",
    "plt.xlabel('Hit Difference Threshold in Number of Strips')\n",
    "plt.ylabel('Efficiency')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PandasGUI INFO — pandasgui.gui — Opening PandasGUI\n",
      "INFO:pandasgui.gui:Opening PandasGUI\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pandasgui.gui.PandasGui at 0x2079f345990>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show(fully_massaged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for each individual TDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metric_for_combo(combo, rpc_heights):\n",
    "    locations = np.array([c['location'] for c in combo])\n",
    "    heights = np.array([rpc_heights[c['rpc']] for c in combo])\n",
    "    uncertainties = np.array([c['uncertainty'] for c in combo])  \n",
    "    weights = 1 / uncertainties**2\n",
    "\n",
    "    try:\n",
    "        coeffs= np.polyfit(heights, locations, 1, cov=False, w=weights)\n",
    "    except np.linalg.LinAlgError:\n",
    "        coeffs = [np.inf, np.inf]\n",
    "        # cov = np.array([[np.inf, np.inf], [np.inf, np.inf]])\n",
    "\n",
    "    slope, intercept = coeffs\n",
    "    # slope_error, intercept_error = np.sqrt(np.diag(cov))\n",
    "    slope_error, intercept_error = 0, 0\n",
    "    predicted = slope * heights + intercept\n",
    "    residuals = locations - predicted\n",
    "    RSS = np.sum(residuals ** 2)\n",
    "\n",
    "    return slope, intercept, slope_error, intercept_error, RSS, combo\n",
    "\n",
    "def analyze_inter_rpc_hit_with_timing_adjusted(df, n):\n",
    "    rpc_time_offsets = {\n",
    "    ('rpc0', 'eta'): (7.94, 12.48),\n",
    "    ('rpc0', 'phi'): (-2.38, 13.69),\n",
    "    ('rpc1', 'eta'): (8.36, 12.22),\n",
    "    ('rpc1', 'phi'): (-3.79, 13.25),\n",
    "    ('rpc2', 'eta'): (8.84, 12.56),\n",
    "    ('rpc2', 'phi'): (-4.35, 13.57),\n",
    "    ('rpc3', 'eta'): (6.86, 12.41),\n",
    "    ('rpc3', 'phi'): (-4.3, 13.96),\n",
    "    ('rpc4', 'eta'): (2.7, 12.37),\n",
    "    ('rpc4', 'phi'): (-7.89, 13.41),\n",
    "    ('rpc5', 'eta'): (2.82, 13.05),\n",
    "    ('rpc5', 'phi'): (9.15, 14.14),\n",
    "}\n",
    "    paths = []\n",
    "\n",
    "    adjusted_muon_speed_cm_ns = 30\n",
    "\n",
    "    rpc_heights = {\n",
    "        'rpc0': 0, \n",
    "        'rpc1': 0.5, \n",
    "        'rpc2': 1.0, \n",
    "        'rpc3': 61.5, \n",
    "        'rpc4': 121.5, \n",
    "        'rpc5': 122.0\n",
    "    }\n",
    "\n",
    "    for event_number, event_group in tqdm(df.groupby('event_number'), desc=\"Processing Events\"):\n",
    "        for direction in ['eta', 'phi']:\n",
    "            direction_group = event_group[event_group['strip_direction'] == direction]\n",
    "            all_clusters = []\n",
    "\n",
    "            unique_rpcs = direction_group['rpc_number'].unique()\n",
    "            if len(unique_rpcs) < n:\n",
    "                break\n",
    "            for rpc in unique_rpcs:\n",
    "                rpc_group = direction_group[direction_group['rpc_number'] == rpc]\n",
    "                for _, row in rpc_group.iterrows():\n",
    "                    location_scaling = 3.09375 if direction == 'eta' else 2.8125\n",
    "                    strip_locations = np.array(row['locations'])\n",
    "                    non_zero_locations = strip_locations[strip_locations != 0]\n",
    "                    if non_zero_locations.size > 0:\n",
    "                        strip_location = non_zero_locations[0] \n",
    "                    else:\n",
    "                        continue \n",
    "\n",
    "                    location = strip_location * location_scaling\n",
    "                    event_time = np.mean(row['times']) - rpc_time_offsets[(rpc, direction)][0]\n",
    "                    cluster_size_scaled = max(row['size'] * location_scaling, location_scaling) / 2\n",
    "                    all_clusters.append({\n",
    "                        'rpc': rpc,\n",
    "                        'location': location,\n",
    "                        'event_time': event_time,\n",
    "                        'uncertainty': cluster_size_scaled,\n",
    "                        'original_location': strip_location\n",
    "                    })\n",
    "\n",
    "            combination_metrics = []\n",
    "            valid_combinations = [] \n",
    "            \n",
    "            for combo in combinations(all_clusters, n):\n",
    "                if len({c['rpc'] for c in combo}) != len(combo):\n",
    "                    continue  # Skip combinations where RPCs are not unique\n",
    "\n",
    "                time_diffs_are_valid = True\n",
    "                for i in range(len(combo)):\n",
    "                    for j in range(i + 1, len(combo)):\n",
    "                        error_window = rpc_time_offsets[(combo[i]['rpc'], direction)][1] + rpc_time_offsets[(combo[j]['rpc'], direction)][1]\n",
    "                        height_diff = abs(rpc_heights[combo[i]['rpc']] - rpc_heights[combo[j]['rpc']])\n",
    "                        time_diff = abs(combo[i]['event_time'] - combo[j]['event_time'])\n",
    "                        expected_time_diff = height_diff / adjusted_muon_speed_cm_ns\n",
    "                        uncertainty_margin = 5\n",
    "\n",
    "                        if not (time_diff <= expected_time_diff + error_window + uncertainty_margin):\n",
    "                            time_diffs_are_valid = False\n",
    "                            break\n",
    "                    if not time_diffs_are_valid:\n",
    "                        break\n",
    "\n",
    "                if not time_diffs_are_valid:\n",
    "                    continue  # Skip to the next combination if time differences are invalid\n",
    "\n",
    "                # Calculate metrics only for valid combinations\n",
    "                metric = calculate_metric_for_combo(combo, rpc_heights)\n",
    "                if metric[4] <= 10:\n",
    "                    valid_combinations.append(metric)\n",
    "                    \n",
    "\n",
    "            for valid_combination in valid_combinations:\n",
    "                paths.append({\n",
    "                    'Event Number': event_number,\n",
    "                    'Direction': direction,\n",
    "                    'Slope': valid_combination[0],\n",
    "                    'Intercept': valid_combination[1],\n",
    "                    'Slope_error': valid_combination[2],\n",
    "                    'Intercept_error': valid_combination[3],\n",
    "                    'Used Coordinates': [(c['rpc'], c['original_location'], c['event_time']) for c in valid_combination[-1]],\n",
    "                    'RSS': valid_combination[4],\n",
    "                })\n",
    "\n",
    "    path_df = pd.DataFrame(paths)\n",
    "    return path_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the timing adjustment using the bottom triplet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metric_for_combo(combo, rpc_heights):\n",
    "    locations = np.array([c['location'] for c in combo])\n",
    "    heights = np.array([rpc_heights[c['rpc']] for c in combo])\n",
    "    uncertainties = np.array([c['uncertainty'] for c in combo])  \n",
    "    weights = 1 / uncertainties**2\n",
    "\n",
    "    try:\n",
    "        coeffs= np.polyfit(heights, locations, 1, cov=False, w=weights)\n",
    "    except np.linalg.LinAlgError:\n",
    "        coeffs = [np.inf, np.inf]\n",
    "        # cov = np.array([[np.inf, np.inf], [np.inf, np.inf]])\n",
    "\n",
    "    slope, intercept = coeffs\n",
    "    # slope_error, intercept_error = np.sqrt(np.diag(cov))\n",
    "    slope_error, intercept_error = 0, 0\n",
    "    predicted = slope * heights + intercept\n",
    "    residuals = locations - predicted\n",
    "    RSS = np.sum(residuals ** 2)\n",
    "\n",
    "    return slope, intercept, slope_error, intercept_error, RSS, combo\n",
    "\n",
    "def analyze_inter_rpc_hit_with_timing_adjusted(df,n):\n",
    "\n",
    "    rpc_time_offsets = {\n",
    "    ('rpc0', 'eta'): (0, 5),\n",
    "    ('rpc0', 'phi'): (0, 5),\n",
    "    ('rpc1', 'eta'): (0, 5),\n",
    "    ('rpc1', 'phi'): (0, 5),\n",
    "    ('rpc2', 'eta'): (0, 5),\n",
    "    ('rpc2', 'phi'): (0, 5),\n",
    "    ('rpc3', 'eta'): (0, 5),\n",
    "    ('rpc3', 'phi'): (0, 5),\n",
    "    ('rpc4', 'eta'): (0, 5),\n",
    "    ('rpc4', 'phi'): (0, 5),\n",
    "    ('rpc5', 'eta'): (0, 5),\n",
    "    ('rpc5', 'phi'): (0, 5),\n",
    "}\n",
    "    paths = []\n",
    "\n",
    "    adjusted_muon_speed_cm_ns = 30\n",
    "\n",
    "    rpc_heights = {\n",
    "        'rpc0': 0, \n",
    "        'rpc1': 0.5, \n",
    "        'rpc2': 1.0, \n",
    "        'rpc3': 61.5, \n",
    "        'rpc4': 121.5, \n",
    "        'rpc5': 122.0\n",
    "    }\n",
    "\n",
    "    for event_number, event_group in tqdm(df.groupby('event_number'), desc=\"Processing Events\"):\n",
    "        for direction in ['eta', 'phi']:\n",
    "            direction_group = event_group[event_group['strip_direction'] == direction]\n",
    "            all_clusters = []\n",
    "\n",
    "            unique_rpcs = direction_group['rpc_number'].unique()\n",
    "            if len(unique_rpcs) < 5:\n",
    "                break\n",
    "            for rpc in unique_rpcs:\n",
    "                rpc_group = direction_group[direction_group['rpc_number'] == rpc]\n",
    "                for _, row in rpc_group.iterrows():\n",
    "                    location_scaling = 3.09375 if direction == 'eta' else 2.8125\n",
    "                    strip_locations = np.array(row['locations'])\n",
    "                    non_zero_locations = strip_locations[strip_locations != 0]\n",
    "                    if non_zero_locations.size > 0:\n",
    "                        strip_location = non_zero_locations[0] \n",
    "                    else:\n",
    "                        continue \n",
    "\n",
    "                    location = strip_location * location_scaling\n",
    "                    event_time = np.mean(row['times']) - rpc_time_offsets[(rpc, direction)][0]\n",
    "                    cluster_size_scaled = max(row['size'] * location_scaling, location_scaling) / 2\n",
    "                    all_clusters.append({\n",
    "                        'rpc': rpc,\n",
    "                        'location': location,\n",
    "                        'event_time': event_time,\n",
    "                        'uncertainty': cluster_size_scaled,\n",
    "                        'original_location': strip_location\n",
    "                    })\n",
    "\n",
    "            combination_metrics = []\n",
    "            valid_combinations = [] \n",
    "            \n",
    "            for combo in combinations(all_clusters, n):\n",
    "                if len({c['rpc'] for c in combo}) != len(combo):\n",
    "                    continue  # Skip combinations where RPCs are not unique\n",
    "                used_rpcs_set = {c['rpc'] for c in combo}\n",
    "  \n",
    "\n",
    "                time_diffs_are_valid = True\n",
    "                for i in range(len(combo)):\n",
    "                    for j in range(i + 1, len(combo)):\n",
    "                        error_window = rpc_time_offsets[(combo[i]['rpc'], direction)][1] + rpc_time_offsets[(combo[j]['rpc'], direction)][1]\n",
    "                        height_diff = abs(rpc_heights[combo[i]['rpc']] - rpc_heights[combo[j]['rpc']])\n",
    "                        time_diff = abs(combo[i]['event_time'] - combo[j]['event_time'])\n",
    "                        expected_time_diff = height_diff / adjusted_muon_speed_cm_ns\n",
    "                        uncertainty_margin = 5\n",
    "\n",
    "                        if not (time_diff <= expected_time_diff + error_window + uncertainty_margin):\n",
    "                            time_diffs_are_valid = False\n",
    "                            break\n",
    "                    if not time_diffs_are_valid:\n",
    "                        break\n",
    "\n",
    "                if not time_diffs_are_valid:\n",
    "                    continue  # Skip to the next combination if time differences are invalid\n",
    "                \n",
    "                if used_rpcs_set.issubset({'rpc0', 'rpc1', 'rpc2'}):\n",
    "                    metric = calculate_metric_for_combo(combo, rpc_heights)\n",
    "                    if metric[4] <= 10:\n",
    "                        valid_combinations.append(metric)\n",
    "                    \n",
    "\n",
    "            for valid_combination in valid_combinations:\n",
    "                paths.append({\n",
    "                    'Event Number': event_number,\n",
    "                    'Direction': direction,\n",
    "                    'Slope': valid_combination[0],\n",
    "                    'Intercept': valid_combination[1],\n",
    "                    'Slope_error': valid_combination[2],\n",
    "                    'Intercept_error': valid_combination[3],\n",
    "                    'Used Coordinates': [(c['rpc'], c['original_location'], c['event_time']) for c in valid_combination[-1]],\n",
    "                    'RSS': valid_combination[4],\n",
    "                })\n",
    "\n",
    "    path_df = pd.DataFrame(paths)\n",
    "    return path_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Events: 100%|██████████| 94226/94226 [00:49<00:00, 1892.94it/s]\n"
     ]
    }
   ],
   "source": [
    "df_Triplet_timing = analyze_inter_rpc_hit_with_timing_adjusted(fully_massaged_df, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PandasGUI INFO — pandasgui.gui — Opening PandasGUI\n",
      "INFO:pandasgui.gui:Opening PandasGUI\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pandasgui.gui.PandasGui at 0x239120fbe20>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show(df_Triplet_timing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
